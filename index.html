<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Compress3D</title>
    <link rel="stylesheet" type="text/css" href="./css/style.css">
</head>
<body>

<!--Title-->
<div class="title">
    <h1>Compress3D: a Compressed Latent Space for 3D Generation from a Single Image</h1>
</div>

<div class="seg">
    <hr class="hr-edge-weak">
</div>

<!--Abstract-->
<div class="paragraph">
    <h1>Abstract</h1>
    <p>3D generation has witnessed significant advancements, yet efficiently producing high-quality 3D assets from a single image remains challenging. In this paper, we present a triplane autoencoder, which encodes 3D models into a compact triplane latent space to effectively compress both the 3D geometry and texture information. Within the autoencoder framework, we introduce a 3D-aware cross-attention mechanism, which utilizes low-resolution latent representations to query features from a high-resolution 3D feature volume, thereby enhancing the representation capacity of the latent space. Subsequently, we train a diffusion model on this refined latent space. In contrast to solely relying on image embedding for 3D generation, our proposed method advocates for the simultaneous utilization of both image embedding and shape embedding as conditions. Specifically, the shape embedding is estimated via a diffusion prior model conditioned on the image embedding. Through comprehensive experiments, we demonstrate that our method outperforms state-of-the-art algorithms, achieving superior performance while requiring less training data and time. Our approach enables the generation of high-quality 3D assets in merely 7 seconds on a single A100 GPU.
    </p>
</div>

<div class="seg">
    <hr class="hr-edge-weak">
</div>

<div class="paragraph">
    <h1>Method Overview</h1>
    <p> </p>
</div>

<div class="figure_custom_width">
    <img src="./figures/Method Overview.png" width="80%">
    <p>Compress3D mainly contains 3 components. (a) Triplane AutoEncoder:  Triplane Encoder encodes color point cloud on a low-resolution triplane latent space. Then we use a Triplane Decoder to decode 3D model from a triplane latent. (b) Triplane Diffusion Model: we use shape embedding and image embedding as conditions to generate triplane latent. (c) Diffusion Prior Model: generate shape embedding conditioned on the image embedding.
    </p>
</div>

<div class="seg">
    <hr class="hr-edge-weak">
</div>
<div class="paragraph">
    <h1> Comparison with Other Methods</h1>
    <p> Compared with other methods, Compress3D can generate 3D models with good texture and fine geometric details.</p>
</div>

<body style="text-align: center;">
    <video width="1080" height="750" loop="true" autoplay muted>
        <source src="./videos/concat_comparison_caption.mp4" type="video/mp4">
    </video>
</body>


<div class="seg">
    <hr class="hr-edge-weak">
</div>
<div class="paragraph">
    <h1> More Results</h1>
    <p> </p>
</div>

<body style="text-align: center;">
    <video width="1080" height="128" loop="true" autoplay muted>
        <source src="./videos/concat_1.mp4" type="video/mp4">
    </video>
</body>
<body style="text-align: center;">
    <video width="1080" height="256" loop="true" autoplay muted>
        <source src="./videos/concat_2.mp4" type="video/mp4">
    </video>
</body>
<body style="text-align: center;">
    <video width="1080" height="256" loop="true" autoplay muted>
        <source src="./videos/concat_3.mp4" type="video/mp4">
    </video>
</body>
<body style="text-align: center;">
    <video width="1080" height="256" loop="true" autoplay muted>
        <source src="./videos/concat_4.mp4" type="video/mp4">
    </video>
</body>
<body style="text-align: center;">
    <video width="1080" height="384" loop="true" autoplay muted>
        <source src="./videos/concat_5.mp4" type="video/mp4">
    </video>
</body>
<body style="text-align: center;">
    <video width="1080" height="256" loop="true" autoplay muted>
        <source src="./videos/concat_6.mp4" type="video/mp4">
    </video>
</body>
<body style="text-align: center;">
    <video width="1080" height="256" loop="true" autoplay muted>
        <source src="./videos/concat_7.mp4" type="video/mp4">
    </video>
</body>



<div class="seg">
    <hr class="hr-edge-weak">
</div>
<div class="paragraph">
    <h1> Ablation Study on Prior Network</h1>
    <p> To validate the importance of diffusion prior model, we train a triplane diffusion model conditioned only on the image embedding and compare it with our method.</p>
</div>

<body style="text-align: center;">
    <video width="1080" height="256" loop="true" autoplay muted>
        <source src="./videos/ablation.mp4" type="video/mp4">
    </video>
</body>



<div class="seg">
    <hr class="hr-edge-weak">
</div>
<div class="paragraph">
    <h1> In the Wild Images</h1>
    <p> In addition to testing our method on the test dataset, we also test our method on some images outside the dataset. Specifically, we collect some images on the internet as input. The generation results are shown as follows, which demonstrates that our method generalizes well to images in the wild.</p>
</div>

<body style="text-align: center;">
    <video width="1080" height="256" loop="true" autoplay muted>
        <source src="./videos/concat_in_the_wild.mp4" type="video/mp4">
    </video>
</body>














</body>
</html>




















